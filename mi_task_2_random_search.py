# -*- coding: utf-8 -*-
"""MI Task 2 - Random Search.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/168hDULp0wR6E-UFuy2rlqiu317ja7DGq
"""

# Task 2 - Random search
# Implement Random search algorithm. Implementation could be done in either Mathematica or Python.
# Test it using test functions from Task 1.

# Your algorithm must have these inputs:
# Test function
# Input domain of the test function
# Dimension size
# Number of iterations

# The algorithm must provide at the output:
# Best found value of objective function f(x)
# The value of x of the best found f(x)
# Convergence graph of the development of the f(x)

#Implementation of Random Search Algorithm for various Test Functions for single-objective optimization
import random
import numpy as np
import matplotlib.pyplot as plt

def random_search(obj_func, bounds, dimensions, num_iter):
    """
    Random search algorithm to optimize a given objective function.

    Parameters:
    obj_func (function): Objective function to be optimized.
        A callable function that takes a list of real-valued inputs and returns a scalar value.
    bounds (list of tuples): Bounds of the input domain of the objective function.
        A list of tuples containing the lower and upper bounds for each dimension of the input domain.
        For example, if the objective function has two inputs, bounds could be [(0, 1), (0, 2)].
    dimensions (int): Dimension of the input domain of the objective function.
        An integer representing the number of inputs to the objective function.
    num_iter (int): Number of iterations for the algorithm.
        An integer representing the number of iterations to run the algorithm for.

    Returns:
    tuple: Best found value of objective function, value of x of the best found f(x),
           and convergence graph of the development of the f(x)
        A tuple containing the following:
        - The best found value of the objective function (a scalar)
        - The inputs to the objective function that produced the best value (a list of length `dimensions`)
        - A list of length `num_iter` containing the best found value of the objective function at each iteration
    """
    # initialize best_f to infinity so that the first value will always be accepted
    best_f = np.inf
    best_x = None
    convergence = []

    # loop through the number of iterations
    for i in range(num_iter):
        # generate a random point within the given bounds for each input dimension
        x = [random.uniform(bounds[j][0], bounds[j][1]) for j in range(dimensions)]
        # evaluate the objective function at the current point
        f = obj_func(x)
        # if the current value is better than the previous best, update the best values
        if f < best_f:
            best_f = f
            best_x = x
        # append the current best value to the convergence list
        convergence.append(best_f)

    return best_f, best_x, convergence



# Test function 1 - Sphere Function
import random
import numpy as np
import matplotlib.pyplot as plt

def sphere(x):
    """
    Sphere function to be optimized.

    Parameters:
    x (list): List of inputs to the function.

    Returns:
    float: Output of the sphere function.
    """
    return sum([i**2 for i in x])

bounds = [(-5.12, 5.12)]*5 # Bounds of the input domain of the sphere function
dimensions = 5 # Dimension of the input domain of the sphere function
num_iter = 1000 # Number of iterations for the random search algorithm

best_f, best_x, convergence = random_search(sphere, bounds, dimensions, num_iter) # Random search algorithm to optimize sphere function

print("Best found value of objective function f(x):", best_f) # Print the best found value of the objective function
print("The value of x of the best found f(x):", best_x) # Print the value of x of the best found f(x)
plt.plot(convergence) # Plot the convergence graph of the development of f(x)
plt.title("Sphere Function - Convergence Plot")
plt.xlabel("Iteration")
plt.ylabel("Best f(x)")
plt.show() # Show the plot of the convergence graph



# Test function 2 - Rosenbrock Function
import random
import numpy as np
import matplotlib.pyplot as plt

def rosenbrock(x):
    """
    Rosenbrock function to be optimized.

    Parameters:
    x (list): Input values for the function.

    Returns:
    float: Output value of the function.
    """
    return sum([100*(x[i+1]-x[i]**2)**2 + (1-x[i])**2 for i in range(len(x)-1)])

bounds = [(-5, 10)]*2
dimensions = 2
num_iter = 1000

best_f, best_x, convergence = random_search(rosenbrock, bounds, dimensions, num_iter)

print("Best found value of objective function f(x):", best_f)
print("The value of x of the best found f(x):", best_x)
plt.plot(convergence)
plt.title("Rosenbrock Function - Convergence Plot")
plt.xlabel("Iteration")
plt.ylabel("Best f(x)")
plt.show()



# Test function 3 - Rastrigin Function
import random
import numpy as np
import matplotlib.pyplot as plt

# Rastrigin Function: accepts an n-dimensional input vector 'x'
# and returns the value of the Rastrigin function at that point
def rastrigin(x):
    return 10*len(x) + sum([(i**2 - 10*np.cos(2*np.pi*i)) for i in x])

# Define search space for the function
bounds = [(-5.12, 5.12)]*5
dimensions = 5
num_iter = 1000

# Perform random search optimization
best_f, best_x, convergence = random_search(rastrigin, bounds, dimensions, num_iter)

# Print the results
print("Best found value of objective function f(x):", best_f)
print("The value of x of the best found f(x):", best_x)

# Plot the convergence graph
plt.plot(convergence)
plt.title("Rastrigin Function - Convergence Plot")
plt.xlabel("Iteration")
plt.ylabel("Best f(x)")
plt.show()



# Test function 4 - Griewank Function
import random
import numpy as np
import matplotlib.pyplot as plt

# Define the Griewank function
def griewank(x):
    return 1 + sum([(i**2)/4000 for i in x]) - np.prod([np.cos(i/np.sqrt(j+1)) for j,i in enumerate(x)])

# Define the search space bounds, dimensions and number of iterations
bounds = [(-600, 600)]*10
dimensions = 10
num_iter = 1000

# Perform random search to find the best minimum value and corresponding x
best_f, best_x, convergence = random_search(griewank, bounds, dimensions, num_iter)

# Print the best found value and corresponding x
print("Best found value of objective function f(x):", best_f)
print("The value of x of the best found f(x):", best_x)

# Plot the convergence of the best found f(x) over iterations
plt.plot(convergence)
plt.title("Griewank Function - Convergence Plot")
plt.xlabel("Iteration")
plt.ylabel("Best f(x)")
plt.show()



# Test function 5 - Ackley Function
def ackley(x):
    # Calculate sum of square term
    sum_sq_term = -20 * np.exp(-0.2 * np.sqrt(sum([xi**2 for xi in x]) / len(x)))
    # Calculate cosine term
    cos_term = -np.exp(sum([np.cos(2*np.pi*xi) for xi in x]) / len(x))
    # Return the final result
    return sum_sq_term + cos_term + 20 + np.e

# Set bounds, dimensions, and number of iterations
bounds = [(-32.768, 32.768)]*5
dimensions = 5
num_iter = 1000

# Run random search algorithm for Ackley function
best_f, best_x, convergence = random_search(ackley, bounds, dimensions, num_iter)

# Print results and plot convergence graph
print("Best found value of objective function f(x):", best_f)
print("The value of x of the best found f(x):", best_x)
plt.plot(convergence)
plt.title("Ackley Function - Convergence Plot")
plt.xlabel("Iteration")
plt.ylabel("Best f(x)")
plt.show()



# Test function 6 - Styblinski-Tang Function
def styblinski_tang(x):
    # Calculate term 1
    term1 = sum([(i**4) - (16 * i**2) + (5 * i) for i in x])
    # Calculate term 2
    term2 = 0.5 * sum([i**2 for i in x])
    # Return the final result
    return term1 + term2

# Set bounds, dimensions, and number of iterations
bounds = [(-5, 5)]*2
dimensions = 2
num_iter = 1000

# Run random search algorithm for Styblinski-Tang function
best_f, best_x, convergence = random_search(styblinski_tang, bounds, dimensions, num_iter)

# Print results and plot convergence graph
print("Best found value of objective function f(x):", best_f)
print("The value of x of the best found f(x):", best_x)
plt.plot(convergence)
plt.title("Styblinski-Tang Function - Convergence Plot")
plt.xlabel("Iteration")
plt.ylabel("Best f(x)")
plt.show()

